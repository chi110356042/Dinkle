{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9V9tpJEyPSpp"
      },
      "outputs": [],
      "source": [
        "class BiLSTM_layer(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, bidirectional, batch_first=False):\n",
        "        super(BiLSTM_layer, self).__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            bidirectional=bidirectional,\n",
        "            batch_first=batch_first\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(hidden_size, 26)\n",
        "        \n",
        "\n",
        "    def forward(self, inputs):\n",
        "        out, (h_n, c_n) = self.lstm(inputs, None)\n",
        "        outputs = self.fc(torch.mean(h_n.squeeze(0), dim=0))\n",
        "\n",
        "        return outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZLTJZuaxCBX"
      },
      "outputs": [],
      "source": [
        "class DataEncoder(nn.Module):\n",
        "  def __init__(self, input_dim, output_dim, hidden_dim=3,dropout=0.4):\n",
        "    super(DataEncoder, self).__init__()\n",
        "    self.input_dim = input_dim\n",
        "    self.output_dim = output_dim\n",
        "    \n",
        "    self.net = nn.Sequential(nn.Linear(input_dim, hidden_dim),\n",
        "                             nn.ReLU(),\n",
        "                             nn.Dropout(dropout),\n",
        "                             nn.Linear(hidden_dim, output_dim)\n",
        "                            )\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "class minmax_RuleEncoder(nn.Module):\n",
        "  def __init__(self, input_dim, output_dim, hidden_dim=3,dropout=0.4):\n",
        "    super(minmax_RuleEncoder, self).__init__()\n",
        "    self.input_dim = input_dim\n",
        "    self.output_dim = output_dim\n",
        "    self.net = nn.Sequential(nn.Linear(input_dim, hidden_dim),\n",
        "                             nn.ReLU(),\n",
        "                             nn.Dropout(dropout),\n",
        "                             nn.Linear(hidden_dim, output_dim)\n",
        "                            )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "class outbound_RuleEncoder(nn.Module):\n",
        "  def __init__(self, input_dim, output_dim, hidden_dim=3,dropout=0.4):\n",
        "    super(outbound_RuleEncoder, self).__init__()\n",
        "    self.input_dim = input_dim\n",
        "    self.output_dim = output_dim\n",
        "    self.net = nn.Sequential(nn.Linear(input_dim, hidden_dim),\n",
        "                             nn.ReLU(),\n",
        "                             nn.Dropout(dropout),\n",
        "                             nn.Linear(hidden_dim, output_dim)\n",
        "                            )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qoa19CmtxCj3"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self, input_dim, output_dim, minmax_rule_encoder, outbound_rule_encoder, data_encoder, hidden_dim=3, n_layers=1, merge='cat', skip=False, input_type='state'):\n",
        "    super(Net, self).__init__()\n",
        "    self.skip = skip\n",
        "    self.input_type = input_type\n",
        "    self.minmax_rule_encoder = minmax_rule_encoder\n",
        "    self.outbound_rule_encoder = outbound_rule_encoder\n",
        "    self.data_encoder = data_encoder\n",
        "    self.n_layers = n_layers\n",
        "    assert self.minmax_rule_encoder.input_dim ==  self.data_encoder.input_dim\n",
        "    assert self.minmax_rule_encoder.output_dim ==  self.data_encoder.output_dim\n",
        "    self.merge = merge\n",
        "    if merge == 'cat':\n",
        "      self.input_dim_decision_block = self.minmax_rule_encoder.output_dim * 3\n",
        "    elif merge == 'add':\n",
        "      self.input_dim_decision_block = self.minmax_rule_encoder.output_dim\n",
        "\n",
        "    self.net = []\n",
        "    for i in range(n_layers):\n",
        "      if i == 0:\n",
        "        in_dim = self.input_dim_decision_block\n",
        "      else:\n",
        "        in_dim = hidden_dim\n",
        "\n",
        "      if i == n_layers-1:\n",
        "        out_dim = output_dim\n",
        "      else:\n",
        "        out_dim = hidden_dim\n",
        "      \n",
        "      self.net.append(BiLSTM_layer(\n",
        "              input_size=in_dim,\n",
        "              hidden_size=64,\n",
        "              num_layers=1,\n",
        "              bidirectional=True,\n",
        "              batch_first=True\n",
        "          ))\n",
        "    self.net = nn.Sequential(*self.net)\n",
        "\n",
        "  def get_z(self, x, alpha=0.0, beta=0.0):\n",
        "    minmax_rule_z = self.minmax_rule_encoder(x)\n",
        "    outbound_rule_z = self.outbound_rule_encoder(x)\n",
        "    data_z = self.data_encoder(x)\n",
        "\n",
        "    if self.merge=='add':\n",
        "      z = alpha * minmax_rule_z + beta * outbound_rule_z + (1-alpha-beta) * data_z    \n",
        "    elif self.merge=='cat':\n",
        "      z = torch.cat((alpha*minmax_rule_z , beta*outbound_rule_z , (1-alpha-beta)*data_z), dim=-1)   \n",
        "\n",
        "    return z\n",
        "\n",
        "  def forward(self, x, alpha=0.0, beta=0.0):\n",
        "    minmax_rule_z = self.minmax_rule_encoder(x)\n",
        "    outbound_rule_z = self.outbound_rule_encoder(x)\n",
        "    data_z = self.data_encoder(x)\n",
        "\n",
        "    if self.merge=='add':\n",
        "      z = alpha*minmax_rule_z + beta*outbound_rule_z + (1-alpha-beta)*data_z   \n",
        "    elif self.merge=='cat':\n",
        "      z = torch.cat((alpha*minmax_rule_z , beta*outbound_rule_z , (1-alpha-beta)*data_z), dim=-1)  \n",
        "    else:\n",
        "      print(self.merge)\n",
        "  \n",
        "    if self.skip:\n",
        "      if self.input_type == 'seq':\n",
        "        return self.net(z) + x[:,-1,:]\n",
        "      else:\n",
        "        return self.net(z) + x    \n",
        "    else:\n",
        "      return self.net(z) \n",
        "      \n",
        "      try:\n",
        "        return self.net(z)   \n",
        "      except:\n",
        "        print(type(z))\n",
        "        print(z.shape)\n",
        "\n",
        "class DataonlyNet(nn.Module):\n",
        "  def __init__(self, input_dim, output_dim, data_encoder, hidden_dim=4, n_layers=2, skip=False, input_type='state'):\n",
        "    super(DataonlyNet, self).__init__()\n",
        "    self.skip = skip\n",
        "    self.input_type = input_type\n",
        "    self.data_encoder = data_encoder\n",
        "    self.n_layers = n_layers\n",
        "    self.input_dim_decision_block = self.data_encoder.output_dim\n",
        "\n",
        "    self.net = []\n",
        "    for i in range(n_layers):\n",
        "      if i == 0:\n",
        "        in_dim = self.input_dim_decision_block\n",
        "      else:\n",
        "        in_dim = hidden_dim\n",
        "\n",
        "      if i == n_layers-1:\n",
        "        out_dim = output_dim\n",
        "      else:\n",
        "        out_dim = hidden_dim\n",
        "\n",
        "      self.net.append(BiLSTM_layer(\n",
        "            input_size=in_dim,\n",
        "            hidden_size=64,\n",
        "            num_layers=1,\n",
        "            bidirectional=True,\n",
        "            batch_first=True\n",
        "        ))\n",
        "      \n",
        "    self.net = nn.Sequential(*self.net)\n",
        "\n",
        "  def get_z(self, x, alpha=0.0):\n",
        "    data_z = self.data_encoder(x)\n",
        "\n",
        "    return data_z\n",
        "\n",
        "  def forward(self, x, alpha=0.0):\n",
        "    data_z = self.data_encoder(x)\n",
        "    z = data_z\n",
        "\n",
        "    if self.skip:\n",
        "      if self.input_type == 'seq':\n",
        "        return self.net(z) + x[:,-1,:]\n",
        "      else:\n",
        "        return self.net(z) + x    \n",
        "    else:\n",
        "      return self.net(z)    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e49f-MQISN1U",
        "outputId": "4d5a5c11-ce02-4396-84bc-2edde9f78b62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (minmax_rule_encoder): minmax_RuleEncoder(\n",
            "    (net): Sequential(\n",
            "      (0): Linear(in_features=3, out_features=64, bias=True)\n",
            "      (1): ReLU()\n",
            "      (2): Dropout(p=0.2, inplace=False)\n",
            "      (3): Linear(in_features=64, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (outbound_rule_encoder): outbound_RuleEncoder(\n",
            "    (net): Sequential(\n",
            "      (0): Linear(in_features=3, out_features=64, bias=True)\n",
            "      (1): ReLU()\n",
            "      (2): Dropout(p=0.2, inplace=False)\n",
            "      (3): Linear(in_features=64, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (data_encoder): DataEncoder(\n",
            "    (net): Sequential(\n",
            "      (0): Linear(in_features=3, out_features=64, bias=True)\n",
            "      (1): ReLU()\n",
            "      (2): Dropout(p=0.2, inplace=False)\n",
            "      (3): Linear(in_features=64, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (net): Sequential(\n",
            "    (0): BiLSTM_layer(\n",
            "      (lstm): LSTM(6, 64, batch_first=True, bidirectional=True)\n",
            "      (fc): Linear(in_features=64, out_features=26, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "merge = 'cat'\n",
        "\n",
        "input_dim =3\n",
        "input_dim_encoder=3\n",
        "output_dim_encoder=2\n",
        "hidden_dim_encoder=64\n",
        "hidden_dim_db=64\n",
        "output_dim_encoder = output_dim_encoder\n",
        "hidden_dim_encoder = hidden_dim_encoder\n",
        "hidden_dim_db =hidden_dim_db\n",
        "output_dim = 26\n",
        "n_layers=1\n",
        "use_type=''\n",
        "\n",
        "\n",
        "outbound_rule_encoder = outbound_RuleEncoder(input_dim, output_dim_encoder, hidden_dim_encoder,dropout=0.2)\n",
        "minmax_rule_encoder = minmax_RuleEncoder(input_dim, output_dim_encoder, hidden_dim_encoder,dropout=0.2)\n",
        "data_encoder = DataEncoder(input_dim, output_dim_encoder, hidden_dim_encoder,dropout=0.2)\n",
        "if use_type == 'no_rule':\n",
        "  model = DataonlyNet(input_dim, output_dim, data_encoder, hidden_dim=hidden_dim_db, n_layers=n_layers)\n",
        "else:\n",
        "  model = Net(input_dim, output_dim, minmax_rule_encoder, outbound_rule_encoder, data_encoder, hidden_dim=hidden_dim_db, n_layers=n_layers, merge=merge)\n",
        "\n",
        "print(model)\n",
        "#optimizer = optim.RMSprop(model.parameters(), lr=0.001,  eps=1e-08, weight_decay=0, momentum=0, centered=False)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)        \n",
        " \n",
        "\n",
        "def custom_mse(y_true, y_pred, alpha, beta):\n",
        "    alpha_loss=alpha\n",
        "    beta_loss=beta\n",
        "    \n",
        "    loss_task = nn.MSELoss()(y_true, y_pred)\n",
        "    #loss_rule=  K.relu(y_pred[:,0] - 3.35)+K.relu(3.30 - y_pred[:,0])+K.relu(y_pred[:,1] - 2.3)+K.relu(2.2 - y_pred[:,1])+K.relu(y_pred[:,2] - 6.2)+K.relu(6.14 - y_pred[:,2])+K.relu(y_pred[:,3] - 2.77)+K.relu(2.63 - y_pred[:,3])+K.relu(y_pred[:,4] - 2.3)+K.relu(2.1 - y_pred[:,4])+K.relu(y_pred[:,5] - 3.37)+K.relu(3.23 - y_pred[:,5])+K.relu(y_pred[:,6] - 2.54)+K.relu(2.34 - y_pred[:,6])+K.relu(y_pred[:,7] - 0.42)+K.relu(0.38 - y_pred[:,7])+K.relu(y_pred[:,8] - 0.63)+K.relu(0.53 - y_pred[:,8])+K.relu(y_pred[:,9] - 0.63)+K.relu(0.53 - y_pred[:,9])+K.relu(y_pred[:,10] - 0.63)+K.relu(0.53 - y_pred[:,10])+K.relu(y_pred[:,11] - 0.63)+K.relu(0.53 - y_pred[:,11])+K.relu(y_pred[:,12] - 0.63)+K.relu(0.53 - y_pred[:,12])+K.relu(y_pred[:,13] - 3.35)+K.relu(3.30 - y_pred[:,13])+K.relu(y_pred[:,14] - 2.3)+K.relu(2.2 - y_pred[:,14])+K.relu(y_pred[:,15] - 6.2)+K.relu(6.14 - y_pred[:,15])+K.relu(y_pred[:,16] - 2.77)+K.relu(2.63 - y_pred[:,16])+K.relu(y_pred[:,17] - 2.3)+K.relu(2.1 - y_pred[:,17])+K.relu(y_pred[:,18] - 3.37)+K.relu(3.23 - y_pred[:,18])+K.relu(y_pred[:,19] - 2.54)+K.relu(2.34 - y_pred[:,19])+K.relu(y_pred[:,20] - 0.42)+K.relu(0.38 - y_pred[:,20])+K.relu(y_pred[:,21] - 0.63)+K.relu(0.53 - y_pred[:,21])+K.relu(y_pred[:,22] - 0.63)+K.relu(0.53 - y_pred[:,22])+K.relu(y_pred[:,23] - 0.63)+K.relu(0.53 - y_pred[:,23])+K.relu(y_pred[:,24] - 0.63)+K.relu(0.53 - y_pred[:,24])+K.relu(y_pred[:,25] - 0.63)+K.relu(0.53 - y_pred[:,25])\n",
        "    loss_rule_minmax= F.relu(y_pred[:,13] - y_pred[:,0])+F.relu(y_pred[:,14] - y_pred[:,1])+F.relu(y_pred[:,15] - y_pred[:,2])+F.relu(y_pred[:,16] - y_pred[:,3])+F.relu(y_pred[:,17] - y_pred[:,4])+F.relu(y_pred[:,18] - y_pred[:,5])+F.relu(y_pred[:,19] - y_pred[:,6])+F.relu(y_pred[:,20] - y_pred[:,7])+F.relu(y_pred[:,21] - y_pred[:,8])+F.relu(y_pred[:,22] - y_pred[:,9])+F.relu(y_pred[:,23] - y_pred[:,10])+F.relu(y_pred[:,24] - y_pred[:,11])+F.relu(y_pred[:,25] - y_pred[:,12])\n",
        "    loss_rule_outbound=F.relu((y_true[:,0] - 3.35)*(3.35-y_pred[:,0]))+F.relu((y_true[:,1] - 2.3)*(2.3-y_pred[:,1]))+F.relu((y_true[:,2] - 6.3)*(6.3-y_pred[:,2]))+F.relu((y_true[:,3] - 2.77)*(2.77-y_pred[:,3]))+F.relu((y_true[:,4] - 2.3)*(2.3-y_pred[:,4]))+F.relu((y_true[:,5] - 3.37)*(3.37-y_pred[:,5]))+F.relu((y_true[:,6] - 2.54)*(2.54-y_pred[:,6]))+F.relu((y_true[:,7] - 0.42)*(0.42-y_pred[:,7]))+F.relu((y_true[:,8] - 0.63)*(0.63-y_pred[:,8]))+F.relu((y_true[:,9] - 0.63)*(0.63-y_pred[:,9]))+F.relu((y_true[:,10] - 0.63)*(0.63-y_pred[:,10]))+F.relu((y_true[:,11] - 0.63)*(0.63-y_pred[:,11]))+F.relu((y_true[:,12] - 0.63)*(0.63-y_pred[:,12]))+F.relu((y_true[:,13] - 3.25)*(3.25-y_pred[:,13]))+F.relu((y_true[:,14] - 2.2)*(2.2-y_pred[:,14]))+F.relu((y_true[:,15] - 6.14)*(6.14-y_pred[:,15]))+F.relu((y_true[:,16] - 2.63)*(2.63-y_pred[:,16]))+F.relu((y_true[:,17] - 2.1)*(2.1-y_pred[:,17]))+F.relu((y_true[:,18] - 3.23)*(3.23-y_pred[:,18]))+F.relu((y_true[:,19] - 2.34)*(2.34-y_pred[:,19]))+F.relu((y_true[:,20] - 2.38)*(2.38-y_pred[:,20]))+F.relu((y_true[:,21] - 0.53)*(0.53-y_pred[:,21]))+F.relu((y_true[:,22] - 0.53)*(0.53-y_pred[:,22]))+F.relu((y_true[:,23] - 0.53)*(0.53-y_pred[:,23]))+F.relu((y_true[:,24] - 0.53)*(0.53-y_pred[:,24]))+F.relu((y_true[:,25] - 0.53)*(0.53-y_pred[:,25]))\n",
        "    #scale = loss_rule_minmax.item() / loss_rule_outbound.item()\n",
        "    #loss_rule = alpha_loss*loss_rule_minmax + scale* beta_loss*loss_rule_outbound\n",
        "    loss_rule_outbound=loss_rule_outbound*10\n",
        "    #scale2 = loss_rule.item() / loss_task.item()\n",
        "    #loss = loss_rule+ scale2*(1-beta_loss-alpha_loss)*loss_task\n",
        "\n",
        "    #print('alpha_loss',alpha_loss)\n",
        "    #print('beta_loss',beta_loss)\n",
        "    loss= alpha_loss*loss_rule_minmax + beta_loss*loss_rule_outbound + (1-beta_loss-alpha_loss)*loss_task\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "ding_model",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}